name: Frontend CD

on:
  push:
    # branches:
    #   - main
    # paths:
    #   - 'starter/frontend/**'
  workflow_dispatch:

jobs:
  npm:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4


    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '20'
    

    - name: Install Dependencies
      run: |
        cd starter/frontend
        npm ci

    # - name: Configure AWS Credentials
    #     run: |
    #       echo "[default]" > ~/.aws/credentials
    #       echo "aws_access_key_id=${{ secrets.ACCESS_KEY }}" >> ~/.aws/credentials
    #       echo "aws_secret_access_key=${{ secrets.SECRET_KEY }}" >> ~/.aws/credentials

    
  
  lint:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4

    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '20'

    - name: Install Dependencies
      run: |
        cd starter/frontend
        npm install

    - name: Install ESLint
      run: |
        cd starter/frontend
        npm install eslint --save-dev
        
    - name: Lint Code
      run: |
        cd starter/frontend
        npm run lint

  test:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4

    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '20'

    - name: Install Dependencies
      run: |
        cd starter/frontend
        npm ci

    - name: Run Tests
      run: |
        cd starter/frontend
        npm test

    - name: Simulate running tests
      run: |
        cd starter/frontend
        CI=true npm test

  build:
    runs-on: ubuntu-latest
    # environment: test
    needs: [lint, test]  # This ensures 'build' only runs if 'lint' and 'test' jobs succeed

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
    
   

    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '20'

    - name: Install Dependencies
      run: |
        cd starter/frontend
        npm ci
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1
  
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
  
    - name: Build, tag, and push image to Amazon ECR
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        ECR_REPOSITORY: frontend
        IMAGE_TAG: ${{ github.sha }}
      run: |
          aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 947772818907.dkr.ecr.us-east-1.amazonaws.com
          cd starter/frontend/
          docker build -t frontend .
          docker tag frontend:latest 947772818907.dkr.ecr.us-east-1.amazonaws.com/frontend:latest
          docker push 947772818907.dkr.ecr.us-east-1.amazonaws.com/frontend:latest

    - name: Install kubectl
      run: |
        curl -sLO https://storage.googleapis.com/kubernetes-release/release/v1.19.9/bin/linux/amd64/kubectl
        chmod +x ./kubectl
        sudo mv ./kubectl /usr/local/bin/
    
    - name: Check kubectl version
      run: kubectl version --client
        
    - name: Display kubectl help
      run: kubectl help
    
    - name: Install Kustomize
      run: |
        curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
        sudo mv kustomize /usr/local/bin/
    

    - name: Check Kustomize version
      run: kustomize version

    - name: Update AWS CLI
      run: |
        sudo apt-get install -y awscli
        aws --version

    - name: Add AWS EKS cluster to kubeconfig
      run: |
        # Specify AWS EKS cluster details
        AWS_REGION=us-east-1
        EKS_CLUSTER_NAME=cluster

        # Set the path for the kubeconfig file
        KUBECONFIG_PATH=/home/runner/.kube/config

        # Update kubeconfig with AWS EKS cluster
        aws eks --region ${AWS_REGION} update-kubeconfig --name ${EKS_CLUSTER_NAME} 
        

        # Display the updated kubeconfig content (optional, for verification)
        cat $KUBECONFIG_PATH

      env:
        KUBECONFIG: /home/runner/.kube/config

    - name: Install and configure aws-iam-authenticator
      run: |
        curl -X GET -L https://github.com/kubernetes-sigs/aws-iam-authenticator/releases/download/v0.6.2/aws-iam-authenticator_0.6.2_linux_amd64 -o aws-iam-authenticator
        chmod +x /home/runner/work/cd12354-Movie-Picture-Pipeline/cd12354-Movie-Picture-Pipeline/aws-iam-authenticator
        pwd       

      env:
        KUBECONFIG: /home/runner/.kube/config
  
    # - name: Fetch IAM GitHub Action user ARN
    #   id: fetch-user-arn
    #   run: |
    #     userarn=$(aws iam get-user --user-name github-action-user | jq -r .User.Arn)
    #     echo "::set-output name=userarn::$userarn"

    - name: Add user to Kubernetes
      run: |
        userarn=$(aws iam get-user --user-name github-action-user | jq -r .User.Arn)
        /home/runner/work/cd12354-Movie-Picture-Pipeline/cd12354-Movie-Picture-Pipeline/aws-iam-authenticator help
        /home/runner/work/cd12354-Movie-Picture-Pipeline/cd12354-Movie-Picture-Pipeline/aws-iam-authenticator add user --userarn="${userarn}" --username=github-action-role --groups=system:masters --kubeconfig=/home/runner/.kube/config --prompt=false
        # Update kubeconfig with AWS EKS cluster
        kubectl get pods



      env:
        KUBECONFIG: /home/runner/.kube/config

      

    
        # aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        # aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        # aws-region: us-east-1

    # - name: Get Pods
    #   run: |
    #     # Specify AWS EKS cluster details
    #     AWS_REGION=us-east-1
    #     EKS_CLUSTER_NAME=cluster

    #     # Set the path for the kubeconfig file
    #     KUBECONFIG_PATH=/home/runner/.kube/config

    #     export AWS_DEFAULT_REGION=$AWS_REGION
    #     export AWS_REGION=$AWS_REGION
    #     export AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}
    #     export AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}

    #     # Update kubeconfig with AWS EKS cluster
    #       kubectl get pods

    # env:
    #   KUBECONFIG: /home/runner/.kube/config

    # - name: Update Kube-config
    #   run: kubectl get pods
    #   env:
    #     KUBECONFIG: /home/runner/.kube/config
    
    # - name: Install kubectl
    #   uses: azure/k8s-set-context@v1
    #   with:
    #     kubectl-version: "1.19.9"
    #     kubeconfig: starter/frontend/
    
    # - name: Update kubeconfig
    #   run: kubectl config view --flatten > $KUBECONFIG
      

    # - name: Update Kube-config
    #   run: |
    #     cd starter/frontend/k8s
    #     aws eks --region us-east-1 update-kubeconfig --name cluster
    
    # - name: Install kustomize
    #   run: |
    #     curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
    #     sudo mv kustomize /usr/local/bin/    

    # - name: Install Kustomize
    #   uses: syntaqx/setup-kustomize@v1

    # - name: Deploy to EKS
    #   run: |
    #     cd starter/frontend/k8s
    #     kustomize edit set image frontend=947772818907.dkr.ecr.us-east-1.amazonaws.com/frontend:latest


    # - name: Apply the manifests to the cluster
    #   run: |
    #     cd starter/frontend/k8s
    #     kustomize build | kubectl apply -f - --validate=false
